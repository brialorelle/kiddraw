---
title: |
  | Developmental changes in the ability to draw 
  | distinctive features of object categories
bibliography: kiddraw_2019.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Bria Long (bria@stanford.edu)} \\ Department of Psychology, 450 Serra Mall \\ Stanford, CA 94305
    \AND {\large \bf Judith E. Fan (jefan@ucsd.edu)} \\Department of Psychology, 9500 Gilman Drive \\ La Jolla, CA 92093
    \AND {\large \bf Zixian Chai (zchai14@stanford.edu)} \\Department of Psychology, 450 Serra Mall \\ Stanford, CA 94305
    \AND {\large \bf Michael C. Frank (mcfrank@stanford.edu)} \\Department of Psychology, 450 Serra Mall \\ Stanford, CA 94305}

abstract: > 
  How do children's visual concepts change across childhood, and how might
  these changes be reflected in their drawings? Here we    
  investigate developmental changes in children’s ability to emphasize the relevant  
  visual distinctions between object categories in their drawings. We collected over 13K drawings 
  from children aged 2-10 years via a free-standing drawing station in a children's museum. We
  hypothesized that older children would produce more recognizable drawings, and that
  this gain in recognizability would not be entirely explained by concurrent development
  in visuomotor control. To measure recognizability, we applied a pretrained
  deep convolutional neural network model to extract a high-level feature
  representation of all drawings, and then trained a multi-way linear classifier on these features.
  To measure visuomotor control, we developed an automated procedure to measure their ability to
  accurately trace complex shapes. We found consistent gains in the recognizability of
  drawings across ages that were not fully explained by children's ability to
  trace complex shapes accurately. Furthermore, these gains were accompanied by
  an increase in how distinct different object categories were in feature space.  
  Overall, these results demonstrate that children's drawings include 
  more distinctive visual features as they grow older.

keywords: >
    object representations; child development; visual production; deep neural networks
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(knitr)
library(tidyverse)
library(assertthat)
library(ggthemes)
library(langcog)
library(forcats)
library(egg)
library(gridExtra)
library(reticulate)
library(readr)
library(ggplot2)
library(reshape2)
library(lme4)
library(stringr)
library(viridis)
library(MuMIn)
theme_set(theme_few())
```


# Methods 
## Dataset
### Drawing Station
We installed a drawing station that featured a tablet-based drawing game in a local science museum. Each participant sat in front of a table-mounted touchscreen tablet and drew by moving the tip of their finger across the display. Participants gave consent and indicated their age via checkbox, and no other identifying information was collected; our assumption was that parents would navigate this initial screen for children. To measure fine visuomotor control, each session began with two tracing trials, followed by a copying trial. On each tracing trial, participants were presented with a shape in the center of the display. The first shape was a simple square, and the second was a more complex star-like shape (Figure \ref{fig:tracing-figure}). On the subsequent copying trial, participants were presented with a simple shape (square or circle) in the center of the display for 2s, which then disappeared. They then were asked to copy the shape in the same location it had initially appeared. Next, participants completed up to eight object drawing trials. On each of these trials, participants were verbally cued to draw a particular object category by a video recording of an experimenter (e.g., “What about a dog? Can you draw a dog?”). On all trials, participants had up to 30 seconds to complete their tracing, copy, or drawing. There are 23 common object categories represented in our dataset, which were collected across three bouts of data collection focused on 8 of these objects at a time. These categories were chosen to be familiar to children, to cover a wide range of superordinate categories (e.g., animals, vehicles, manipulable objects), and to vary in the degree to which they are commonly drawn by young children (e.g., trees vs. keys).


```{r load-classifications}
## Load classification data
classification_data <- read.csv('../../data/cogsci_2019/classification-outputs/Classification_Outputs8694.csv') %>%
  as.tibble() %>%
  mutate(session_id = paste('cdm_',session_id,sep="")) %>%
  mutate(age_numeric = age) %>%
  mutate(age = paste('age',age,sep="")) %>%
  mutate(age = as.factor(age)) %>%
  mutate(category = target_label) %>% 
  mutate(image_name = paste(category,'_sketch_', age,'_', session_id,'.png',sep="")) %>%
  select(-X) 
```

```{r load-metadata}
## Load metadata and merge with classificatinos
practice_categories = c('shape','this circle','square','this square','something you love')
extra_prompt = c('something you love')

## Load in meta data from mongo-db database dumps
meta_cdm_run_v4 <- read.csv('../../data/cogsci_2019/mongodb-output/MuseumStation_AllDescriptives_7200_images_cdm_run_v4.csv') %>%
  as.tibble() 

all_meta_data <- read.csv('../../data/cogsci_2019/mongodb-output/MuseumStation_AllDescriptives_20780_images_final_cdm_run_v3.csv') %>%
  as.tibble() %>%
  full_join(meta_cdm_run_v4) %>%
  filter(!category %in% practice_categories) %>%
  filter(!category %in% extra_prompt) %>%
  mutate(category_long = category) %>%
  mutate(category = str_split_fixed(category," ",2)[,2]) %>%
  mutate(draw_duration = draw_duration_old) # use version of drawing duration from 1st to last stroke since same across dataset

## join with classification data
d <- classification_data %>%
  left_join(all_meta_data) 
```

```{r load-tracing}
## Load tracing data
tracing <- read.csv('../../data/cogsci_2019/tracing_eval/tracing_eval.csv') %>%
  mutate(age_numeric = age) %>%
  mutate(age = paste0("age",age_numeric)) %>%
  mutate(age = as.factor(age))

## Extract relevant statistics
tracing_summary <- tracing %>%
  group_by(session_id) %>%
  summarize(avg_spatial = sum(norm_spatial)/n(), avg_shape = sum(norm_shape)/n(), avg_tracing_rating = sum(human_norm_rating)/n()) # use instead of mean because sometimes that errors when we only have one tracing

## Join into one data frame
d <- d %>%
  left_join(tracing_summary, by=c('session_id'))
```

### Dataset Filtering & Descriptives
Given that we could not easily monitor all environmental variables at the drawing station that could impact task engagement (e.g., ambient noise, distraction from other museum visitors), we anticipated the need to develop robust and consistent procedures for data quality assurance. We thus adopted strict screening procedures to ensure that any age-related trends we observed were not due to differences in task compliance across age. Early on, we noticed an unusual degree of sophistication in 2-year-old participants’ drawings and suspected that adult caregivers accompanying these children may not have complied with task instructions to let children draw on their own. Thus, in later versions of the drawing game, we surveyed participants to find out whether another child or an adult had also drawn during the session; all drawings where interference was reported were excluded from analyses. Out of these 2685 participants, 700 filled out the survey, and 156 reported interference from another child or adult (5.81%).  Raw drawing data ($N$ = 15594 drawings) were then screened for task compliance using a combination of manual and automated procedures (i.e., excluding blank drawings, pure scribbles, and drawings containing words), resulting in the exclusion of 15.3% of all drawings (N=13205 drawings after exclusions). After filtering, we analyzed data from `r length(unique(d$session_id))` children who were on average `r round(mean(d$age_numeric),2)` years of age (range 2-10 years).

##  Measuring Tracing Accuracy

We developed an automated procedure for evaluating how accurately participants performed the tracing task, validated against empirical judgments of tracing quality. We decompose tracing accuracy into two terms: a shape error term and a spatial error term. Shape error reflects how closely the participant’s tracing matched the contours of the target shape; the spatial error reflects how closely the location, size, and orientation of the participant’s tracing matched the target shape (Figure \ref{fig:tracing-figure}).

To compute these error terms, we applied an image registration algorithm, Airlab [@sandkuhler2018], to align each tracing to the target shape, yielding an affine transformation matrix minimizing the pixel-wise normalized correlation loss $Loss_{NCC} = - \frac{\sum S \cdot T - \sum E(S) E(T)}{N \sum Var(S) Var(T)}$ between the transformed tracing and the target shape, where $N$ is the number of pixels in both images. 

```{r tracing-figure, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=2, fig.height=3, set.cap.width=T, num.cols.cap=1, fig.cap = "Measurement of tracing task performance reflects both spatial and shape error components. Left: The grey shape is the target; the black shape is the raw tracing. After applying affine image registration, the spatial error reflects the extent of translation, rotation and scaling transformation required to minimize shape error. Right: Shape error reflects how closely the contour of the transformed tracing aligns with the target."}
img <- png::readPNG("figs/tracing_eval.png")
grid::grid.raster(img)
```

The shape error was defined to be the z-scored cross-correlation loss between the transformed tracing and the target shape. The spatial error was defined to be a combination of three sources of error: location, orientation, and size error, derived by decomposing the affine transformation into translation, rotation, and scaling components. The resulting raw translation, rotation, and scaling errors were then z-scored independently within each spatial error dimension, then summed. This sum was z-scored again to yield the combined spatial error. 


```{r}
## Join all tracing ids with classification data 
tracing_ids <- tracing %>%
  filter(session_id %in% d$session_id) # filter to only include sessions in classification

### How many tracings/participants were evaluated overall?
tracing_sessions_evaluated = length(unique(tracing_ids$session_id))
num_tracings_evaluated = length(tracing_ids$session_id)

### How may participants were in mturk study & classification dataset?
tracing_ids_in_mturk <- tracing %>%
  filter(!is.na(human_rating)) %>%
  filter(session_id %in% d$session_id) # filter to only include sessions in classification

tracing_participants_mturk=length(unique(tracing_ids_in_mturk$session_id))
tracings_mturk=length(tracing_ids_in_mturk$session_id)

### How may participants were NOT in mturk study but in the classification dataset?
tracing_ids_remaining <- tracing %>%
  filter(is.na(human_rating)) %>%
  filter(session_id %in% d$session_id) # filter to only include sessions in classification

tracing_participants_not_mturk=length(unique(tracing_ids_remaining$session_id))
tracings_not_mturk=length(tracing_ids_remaining$session_id)
```

Although we assumed that both shape and spatial error terms should contribute to our measure of tracing task performance, we did not know how much weight to assign to each component to best predict empirical judgments of tracing quality.
In order to estimate these weights, we collected quality ratings for 1222 tracings (50-80 tracings x 2 shapes x 9 age categories) from adult observers (N=25); `r tracings_mturk` tracings were taken from the current dataset and the others were taken from a previous experiment as a validation sample. Raters were instructed to evaluate “how well the tracing matches the target shape and is aligned to the position of the target shape” on a 5-point scale. 

To control for individual variation in the extent to which they used the full range of possible ratings, we z-scored ratings within a session to map all sessions to the same scale. We then fit a linear mixed-effects model containing shape error, spatial error, their interaction, and shape identity (square vs. star) as predictors of the z-scored empirical ratings. This model yielded parameter estimates that could then be used to score each tracing in the remainder of the dataset (N=`r tracings_not_mturk` tracings from `r tracing_participants_not_mturk` children). We averaged scores within session to yield a single tracing score for each participant (`r tracing_sessions_evaluated` children completed at least one tracing trial).

## Measuring Object Drawing Recognizability
We also developed an automated procedure for evaluating how well participants included category-diagnostic information in their drawings, by examining classification performance on the features extracted by a deep convolutional neural network model. 

### Visual Encoder
To encode the high-level visual features of each sketch, we used the VGG-19 architecture [@simonyan2014very], a deep convolutional neural network pre-trained on Imagenet classification. We used model activations in the second-to-last layer of this network, which contain more explicit representations of object identity than earlier layers [@FanCommon2018; @yamins2014performance; @long2018drawings]. Raw feature representations in this layer consist of flat 4096-dimensional vectors, to which we applied channel-wise normalization. 

### Logistic Regression Classifier
Next, we used these features to train an object category decoder. To avoid any bias due to imbalance in the distribution of drawings over categories (since groups of categories ran at the station for different times), we sampled such that there were an equal number of drawings of each of the 23 categories (N=8694 drawings total). We then trained a 23-way logistic classifier with L2 regularization under leave-one-out cross-validation to estimate the recognizability of every drawing in our dataset. 

```{r descriptives-across-age}
### How do our covariates change with age? Compute means and CIs; Group by age/category

## first summarize data  
cor_by_age <- d %>%
  group_by(age_numeric,category) %>%
  summarize(avg_cor = mean(correct_or_not)) %>%
  group_by(age_numeric) %>%
  multi_boot_standard(col = "avg_cor")  

draw_duration <- d %>%
  group_by(age_numeric,category) %>%
  summarize(avg_draw_duration = mean(draw_duration)) %>%
  group_by(age_numeric) %>%
  multi_boot_standard(col = "avg_draw_duration")

num_strokes <- d %>%
  group_by(age_numeric,category) %>%
  summarize(avg_num_strokes = mean(num_strokes)) %>%
  group_by(age_numeric) %>%
  multi_boot_standard(col = "avg_num_strokes") 

avg_intensity <- d %>%
  group_by(age_numeric,category) %>%
  summarize(avg_intensity = mean(mean_intensity)) %>%
  group_by(age_numeric) %>%
  multi_boot_standard(col = "avg_intensity")

tracing_scores <- d %>%
  distinct(session_id,age_numeric,avg_tracing_rating) %>%
  filter(!is.na(avg_tracing_rating)) %>%
  group_by(age_numeric) %>%
  multi_boot_standard(col = "avg_tracing_rating")
```

```{r plot-descriptives-across-age}
## Make compiled plot of descriptives
base_size_chosen=18 # size of text in plots
smooth_alpha=.2

cor_by_age_plot_A = ggplot(cor_by_age, aes(age_numeric,mean, color=age_numeric)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) + 
  theme_few(base_size = base_size_chosen) + 
  labs(x='Age', y='Classification accuracy') +
  scale_color_viridis(option="D") + 
  theme(legend.position = "none") + 
  geom_smooth(col='grey',span=10, alpha=smooth_alpha) +
  ggtitle('A') + 
  ylim(0,.75) + 
  geom_hline(yintercept = 1/23, linetype="dashed", color="grey")

p1=ggplot(draw_duration, aes(age_numeric,mean, color=age_numeric)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +
  theme_few(base_size = base_size_chosen) +
  labs(x='Age', y='Draw duration (s)') +
  scale_color_viridis(option="D") +
  theme(legend.position = "none") + 
  ylim(0,15) + 
  geom_smooth(col='grey', span = 10) +
  ggtitle('B')

p2=ggplot(avg_intensity, aes(age_numeric,mean, color=age_numeric)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +
  theme_few(base_size = base_size_chosen) +
  labs(x='Age', y='Ink used (mean intensity)') +
  scale_color_viridis(option="D") +
  theme(legend.position = "none") + 
  ylim(.02,.05) + 
  geom_smooth(col='grey', span = 10,alpha=smooth_alpha)  +
  ggtitle('C')

p3=ggplot(num_strokes, aes(age_numeric,mean, color=age_numeric)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +
  theme_few(base_size = base_size_chosen) +
  labs(x='Age', y='Number of strokes') +
  scale_color_viridis(option="D") +
  theme(legend.position = "none") +
  ylim(0,20) + 
  geom_smooth(col='grey', span = 10,alpha=smooth_alpha)  +
  ggtitle('D')
        
p4=ggplot(tracing_scores, aes(age_numeric,mean, color=age_numeric)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +
  theme_few(base_size = base_size_chosen) +
  labs(x='Age', y='Normalized tracing score') +
  scale_color_viridis(option="D") +
  theme(legend.position = "none") + 
  geom_smooth(col='grey', span = 10,alpha=smooth_alpha)  +
  ggtitle('E')
```

```{r mainResults, include = T, fig.env = "figure*", fig.pos = "h", fig.width=7, fig.height=2, set.cap.width=T, num.cols.cap=2, fig.cap = " (A) Leave-one-out classification accuracy (grey dotted line indicates chance) (B) the amount of time spent drawing in seconds, (C) the amount of ink used (i.e., mean intensity of the drawings), (D) the number of strokes used, and (E) the average normalized tracing scores are plotted as a function of children’s age."}
ggarrange(cor_by_age_plot_A,p1,p2,p3,p4, nrow = 1)
```

```{r}
### Plot results only all items and in ssame order
unfamiliar_list = c('key','sheep','scissors','couch','bear','frog','phone')
              
d <- d %>%
    mutate(unfam_category = (category %in% unfamiliar_list))

cor_by_cat_by_age <- d %>%
  group_by(age_numeric, category, unfam_category) %>%
  multi_boot_standard(col = "correct_or_not") 

new_order = c("house","couch","chair","airplane", "bike","car","boat","train","bear","cat","rabbit", "dog","sheep","bird","frog","fish","person","tree","bowl","phone","cup","scissors","key")
cor_by_cat_by_age$category = fct_relevel(cor_by_cat_by_age$category, new_order)  


ggplot(cor_by_cat_by_age, aes(x=age_numeric,y=mean, col=age_numeric)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), position=position_dodge(width=.5)) + 
  theme_few(base_size = base_size_chosen) + 
  labs(x='Age', y='Classification accuracy') +
  theme(legend.position = "none") +
  scale_color_viridis(option="D") +
  geom_smooth(color='grey',alpha=.2) +
  scale_x_discrete(limits=c(2,3,4,5,6,7,8,9,10)) +
  facet_wrap(~category)
```

```{r}
### Look at familiarity as a factor
cor_by_age_by_fam <- d %>%
  group_by(age_numeric,category,unfam_category) %>%
  summarize(avg_cor = mean(correct_or_not)) %>%
  group_by(age_numeric, unfam_category) %>%
  multi_boot_standard(col = "avg_cor")  

ggplot(cor_by_age_by_fam, aes(age_numeric,mean, color=unfam_category)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), position=position_dodge(width=.5)) + 
  theme_few(base_size = base_size_chosen) + 
  labs(x='Age', y='Classification accuracy') +
  theme(legend.position = "none") +
  scale_x_discrete(limits=c(2,3,4,5,6,7,8,9,10)) 

ggplot(cor_by_age_by_fam, aes(age_numeric,mean, color=unfam_category)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), position=position_dodge(width=.5)) + 
  theme_few(base_size = base_size_chosen) + 
  labs(x='Age', y='Classification accuracy') +
  theme(legend.position = "none") +
  scale_x_discrete(limits=c(2,3,4,5,6,7,8,9,10)) 


```



###  Predicting Object Drawing Recognizability
If children's drawings contain more features that are diagnostic of the drawn categories, then these visual features (estimated via VGG-19) should lead to greater classification accuracy. However, we anticipated that classification accuracy may also vary with children's tracing abilities as well how much time and effort children invested in their drawings; we thus recorded how much time was taken to produce each drawing, how many strokes were drawn, and the proportion of the drawing canvas that was filled. Our main statistical model was then a generalized linear mixed-effects model predicting classification accuracy from the category decoder, with scaled age (in years), tracing score (averaged over both trials), and effort cost variables (i.e., time, strokes, ink) modeled as fixed effects, and with random intercepts for each child and object category.

### Measuring Category Distinctiveness
To investigate changes to the underlying feature representation of children's drawings that may help explain variation in classification accuracy, we computed a measure of pair-wise category distinctiveness $D_{ij}$ for each pair of categories ${i,j}$ within each age. This metric is a higher-dimensional analog of d-prime that incorporates both the distance between each pair of categories as well as the dispersion within each category. We first computed the category centers as the mean feature vector for each category, $\vec{r}_{i}$ and $\vec{r}_{j}$. The distance between each pair of categories ${i,j}$ was then taken  as the Euclidean distance between their category centers, $\lVert\vec{r}_{i}-\vec{r}_{j} \rVert_{2}$. The dispersion for each category was computed as the root-mean-squared Euclidean distance of each individual drawing vector from the category center vector $\vec{r}$ and is expressed as $s$. By direct analogy with d-prime, we compute the distinctiveness $D_{ij}$ of each pair of categories ${i,j}$ by dividing the Euclidean distance between category centers by the quadratic mean of the two category dispersions, $D_{ij} = \frac{\lVert\vec{r}_{i}-\vec{r}_{j} \rVert_{2} } {\sqrt{\frac{1}{2} (s_{i}^2 + s_{j}^2})}$.

```{r}
###
rdm_corr <- read.csv('../../data/cogsci_2019/feature_space_metrics/rdm_corr_by_age_dec13.csv') %>%
  as.tibble() %>%
  mutate(age = X + 2) %>%
  mutate(se_lower = rdm_corr_avg - rdm_corr_sem, se_upper = rdm_corr_avg + rdm_corr_sem)

###
dprime <- read.csv('../../data/cogsci_2019/feature_space_metrics/dprime_by_age_dec13.csv') %>%
  as.tibble() %>%
  mutate(age = X + 2) %>%
  mutate(se_lower = dprime_avg - dprime_sem, se_upper = dprime_avg + dprime_sem)

####
dispersions <- read.csv('../../data/cogsci_2019/feature_space_metrics/class_dispersion_by_age_dec13.csv') %>%
  as.tibble()

dispersions = gather(dispersions, category, rmse, -X)
dispersions <- dispersions %>%
  mutate(age = X + 2) %>%
  select(-X)

#####
expansion <- read.csv('../../data/cogsci_2019/feature_space_metrics/distances_jacknifed_age_means_dec13.csv') %>%
  as.tibble() 

expansion = gather(expansion, category, dist_to_center, -X)
expansion <- expansion %>%
  mutate(age = X + 2) %>%
  select(-X)
```

```{r layerWise, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3, fig.height=3, set.cap.width=T, num.cols.cap=1, fig.cap = "Spearman's correlation between representational dissimilarity matrices (RDMs) of drawings produced by adults vs. other adults, adults vs. older children, and between adults vs. younger children at each layer of VGG-19. "}

new_order = c("house","couch","chair","airplane", "bike","car","boat","train","bear","cat","rabbit", "dog","sheep","bird","frog","fish","person","tree","bowl","phone","cup","scissors","key")
dispersions$category = fct_relevel(dispersions$category, new_order)  

ggplot(dispersions, aes(x = age, y = rmse, col=category)) +
 geom_point(position = position_dodge(width = .1)) +
 geom_line(alpha=.2) +
 theme_few(base_size=18) +
 labs(y = "Within-category dispersions", x = "Age") +
 scale_x_continuous(breaks = c(2,3,4,5,6,7,8,9,10)) +
 scale_y_continuous(breaks = c())

```



```{r}
max_cat_by_age <- dispersions %>%
  group_by(age) %>%
  mutate(max_rmse = max(rmse)) %>%
  filter(rmse == max_rmse)

min_cat_by_age <- dispersions %>%
  group_by(age) %>%
  mutate(min_rmse = min(rmse)) %>%
  filter(rmse == min_rmse)


```
```{r}

ggplot(rdm_corr, aes(x = age, y = rdm_corr_avg)) +
 geom_point(position = position_dodge(width = .1)) +
 geom_pointrange(aes(ymin = se_lower, ymax = se_upper)) +
 geom_line() +
 theme_few(base_size = 18) +
 ylim(.5,1) + 
 labs(y = "Similarity to 10-year-olds (r)", x = "Age") +
 scale_x_continuous(breaks = c(2,3,4,5,6,7,8,9)) 

```




```{r inferential-stats-1, cache=FALSE}
## INFERENTIAL STATS 1- Classification accuracy
accuracy_all_covariates <- glmer(correct_or_not ~ scale(avg_tracing_rating)*scale(age_numeric) +
                          scale(draw_duration) +
                          scale(mean_intensity) +
                          scale(num_strokes) +
                        (1|session_id) +
                        (1|category),
      data = d, family="binomial")

accuracy_all_covariates_no_int <- glmer(correct_or_not ~ scale(avg_tracing_rating)+scale(age_numeric) +
                          scale(draw_duration) +
                          scale(mean_intensity) +
                          scale(num_strokes) +
                        (1|session_id) +
                        (1|category),
      data = d, family="binomial")

# xtable::xtable(summary(accuracy_all_covariates)$coef, digits=3, caption = "Model coefficients of a GLMM predicting the recognziability of each drawing")
```

```{r inferential-stats-2, cache=FALSE}
accuracy_no_age <- glmer(correct_or_not ~ scale(avg_tracing_rating) + scale(draw_duration) +
                          scale(mean_intensity) +
                          scale(num_strokes) +
                        (1|session_id) +
                        (1|category),
      data = d, family="binomial")

accuracy_no_age_or_tracing <- glmer(correct_or_not ~ scale(draw_duration) +
                          scale(mean_intensity) +
                          scale(num_strokes) +
                        (1|session_id) +
                        (1|category),
      data = d, family="binomial")

accuracy_no_tracing <- glmer(correct_or_not ~ scale(age_numeric) +
                          scale(draw_duration) +
                          scale(mean_intensity) +
                          scale(num_strokes) +
                        (1|session_id) +
                        (1|category),
      data = d, family="binomial")


###
null = r.squaredGLMM(accuracy_no_age_or_tracing)
no_age = r.squaredGLMM(accuracy_no_age)
no_tracing = r.squaredGLMM(accuracy_no_tracing)
all = r.squaredGLMM(accuracy_all_covariates)
no_int = r.squaredGLMM(accuracy_all_covariates_no_int) # no_int = no interaction between tracing/age
```




```{r class-accuracy-by-tracing} 
## Compute target_label_prob averages within quantiles / age
num_quantiles = 4

# compute quantiles without knowing about age
d <- d %>%
  mutate(avg_tracing_rating_quantile = ntile(avg_tracing_rating,num_quantiles))

# now compute averages
avg_by_tracing <- d %>%
  filter(!is.na(avg_tracing_rating_quantile)) %>%
  group_by(avg_tracing_rating_quantile,age_numeric) %>%
  multi_boot_standard(col = "correct_or_not")

avg_by_tracing$avg_tracing_rating_quantile = as.factor(avg_by_tracing$avg_tracing_rating_quantile)
levels(avg_by_tracing$avg_tracing_rating_quantile)=c("Tracing Quartile 1","Tracing Quartile 2","Tracing Quartile 3","Tracing Quartile 4")

avg_by_tracing_plot = ggplot(avg_by_tracing, aes(age_numeric,mean, color=avg_tracing_rating_quantile)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper,alpha=.2)) +
  theme_few(base_size = base_size_chosen) +
  labs(x='Age', y='Proportion correct') +
  theme(legend.position = "none") +
  geom_smooth(col='grey', span = 10, method='lm')  +
  facet_grid(~avg_tracing_rating_quantile)
```

```{r tracingResults, include = T, fig.env = "figure*", fig.pos = "h", fig.width=7, fig.height=2, set.cap.width=T, num.cols.cap=2, fig.cap = "Data are divided into four quantiles based on the distribution of tracing scores in the entire dataset; these divisions represent the data in each panel. In each panel, the average probability assigned to the target class is plotted as a function of child’s age.  Error bars represent 95\\% CIs bootstrapped within each age group and subset of tracing scores."}
ggarrange(avg_by_tracing_plot, nrow=1)
```

```{r distinctiveness, include = T, fig.env = "figure*", fig.pos = "h", fig.align='center', fig.width=8, fig.height=2.5, set.cap.width=T, num.cols.cap=2, fig.align = "center", fig.cap = "Pairwise category distinctiveness for drawings made by 3-, 5-, 7-,and 9-year-olds; darker (vs. lighter) values represent pairs of categories that have more overlapping (vs. distinctive) representations."}
examples <- png::readPNG("figs/distinctiveness.png")
grid::grid.raster(examples)
```


# Results
Overall, drawing classification accuracy increased with age (Figure \ref{fig:mainResults}A), validating our basic expectation that older children's drawings would be more recognizable.
Our mixed-effects model on drawing classification revealed that this age-related gain held when accounting for task covariates—-the amount of time spent drawing, the number of strokes, and total ink used (Figure \ref{fig:mainResults}B,C,D)—-and for variation across object categories and individual children. All model coefficients can be found in Table 1.

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & z value & Pr($>$$|$z$|$) \\ 
  \hline
(Intercept) & -0.700 & 0.274 & -2.555 & 0.011 \\ 
  Tracing & 0.312 & 0.034 & 9.143 & 0.000 \\ 
  Age & 0.272 & 0.034 & 8.075 & 0.000 \\ 
  Draw Duration & 0.139 & 0.034 & 4.055 & 0.000 \\ 
  Avg Intensity & -0.064 & 0.033 & -1.936 & 0.053 \\ 
  Num. Strokes & -0.035 & 0.034 & -1.019 & 0.308 \\ 
  Tracing*Age  & -0.017 & 0.028 & -0.613 & 0.540 \\ 
   \hline
\end{tabular}
\caption{Model coefficients of a GLMM predicting the recognizability of each drawing.} 
\end{table}

We next examined the relationship between children's ability to trace complex shapes and the subsequent recognizability of their drawings. Tracing abilities increased with age (Figure \ref{fig:mainResults}E) and individual’s tracing abilities were good predictors of the recognizability of the drawings they produced. This main effect of tracing ability also held when accounting for effort covariates (number of strokes, time spent drawing, ink used). However, children’s tracing abilities did not interact with the age-related gains in classification we observed (Figure \ref{fig:tracingResults}) and we observed age-related classification gains at each level of tracing ability.

To examine the contributions of age and tracing ability to recognizability, we also fit reduced versions of the full model and examined the marginal $R^2$ [@nakagawa2013general]. The fixed effects in a null model without tracing or age (which mainly captures drawing effort) accounted for very little variance (marginal $R^2$ = `r round(null[1,1],3)`). Adding only children's age to the model increased $R^2$ (marginal $R^2$ = `r round(no_tracing[1,1],3)`) as did only adding tracing  (marginal $R^2$ = `r round(no_age[1,1],3)`). Adding both factors without their interaction (marginal $R^2$ = `r round(no_int[1,1],3)`) had a similar effect to adding both factors and their interaction (marginal $R^2$ = `r round(all[1,1],3)`). Attesting to the immense variability between individuals and categories, adding random effects (and many more parameters) accounted for a much larger amount of variance (conditional $R^2$ for full model = `r round(all[1,2],3)`)

These age-related changes in classification accuracy show that the underlying feature representations of older children's drawings were more linearly discriminable. This finding led us investigate a potential source of this enhanced discriminability: that drawings from different categories were spread further apart in feature space, while drawings within a category were clustered closer together. To evaluate this possibility, we used a measure of pair-wise category distinctiveness $D_{ij}$ that accounts for both the distance between each pair of categories, as well as the dispersion within each category. We found that category distinctiveness increased consistently with age (Figure \ref{fig:distinctiveness}). 

Taken together, these results reveal developmental changes in the how well children are able to emphasize the relevant distinctions between object categories in their drawings and thereby support recognition. Moreover, they show that these age-related gains in classification are not entirely explainable by concurrent development in visuomotor control. 

# General Discussion
How do children represent different object categories throughout childhood? Drawings are a rich potential source of information about how visual representations change over development. One possibility is that older children’s drawings are more recognizable because children are better able to include the diagnostic features of particular categories that distinguish them from other similar objects. Supporting this hypothesis, the high-level visual features present in children’s drawings could be used to estimate the category children were intending to draw, and these classifications became more accurate as children became older. These age-related gains in classification were not entirely explainable by either low-level task covariates (e.g., amount of time spent drawing, average intensity, or number of strokes) or children’s tracing abilities. In addition, these gains in classification were paralleled by an increase in the distinctiveness between the categories that children drew (Figure 5).

Taken together, these results suggest that children’s drawings contain more distinctive features as they grow older, perhaps reflecting a change in their internal representations of these categories. While children could simply be learning routines to draw certain categories—perhaps from direct instruction or observation,  our results held even when restricted to a subset of very rarely drawn categories (e.g., “couch”, ”scissors”, ”key”,) arguing against a simple version of this idea.

While the current paper provides a substantial advance over prior work on children's drawings [@long2018drawings], there are nevertheless limitations on the generalizability of these findings due to the nature of our dataset. First, while this dataset is large and samples a heterogenous population, all drawings were collected at a single geographical location, limiting the generalizability of these results to children from other diverse cultural or socioeconomic backgrounds. Second, while we imposed strong filtering requirements on the dataset, we were not present while the children were drawing and thus cannot be sure that we've eliminated all sources of noise or interference. Thus, these correlational results call for validation in more carefully controlled contexts and across more diverse populations.

Furthermore, they open the door for future empirical work to establish causal links between children's drawing behavior to their changing internal representation of visual concepts. For example, it would be valuable to explore the extent to which a child's ability to include the most diagnostic features in their drawings of objects predicts their ability to perceptually discriminate those objects. Another promising direction would be to investigate the relationship between children's general ability to retrieve relevant information from semantic memory (e.g., that a rabbit has long ears and whiskers), and their ability to produce recognizable drawings of those objects. Insofar as such retrieval mechanisms are engaged during drawing production, developmental changes in semantic memory systems may also explain an important portion of the age-related variation in drawing behavior.  

Overall, we suggest that children’s drawings change systematically across development, and that they contain rich information about children’s underlying representations of the categories in the world around them. A full understanding of how children’s drawings reflect their emerging perceptual and conceptual knowledge will allow a unique and novel perspective on the both the development and the nature of visual concepts—the representations that allow us to easily derive meaning from what we see.

# Acknowledgements
Blinded for review. 

# References 
```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
